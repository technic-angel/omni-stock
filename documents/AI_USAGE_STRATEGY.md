# Project Strategy & AI Usage Guidance

**Date**: November 22, 2025
**Context**: You've built Omni-Stock primarily with AI assistance and are now at a crossroads about how to proceed.

---

## üéØ **CORE QUESTION: Should You Continue or Start Over?**

### **My Strong Recommendation: CONTINUE THIS PROJECT**

**Why:**

1. **You Have Strong Foundations**
   - Working architecture (service/selector pattern is solid)
   - Deployed app on Render + Vercel
   - CI/CD pipeline functional
   - 30+ backend tests, 16+ frontend tests
   - Real deployment with actual hosting

2. **Starting Over = Wasted Investment**
   - You've already solved hard problems (deployment, CORS, vendor scoping)
   - The "bones" are excellent - architecture is production-grade
   - You'd hit the same issues again (just with different syntax)
   - Hiring managers care about RESULTS, not process

3. **The Tipping Point Is NOW**
   - You mentioned: "I figure we are right around the tipping point of me actually coding around just as much as AI code generates"
   - **This is EXACTLY when you should double down**
   - You've learned the patterns - now reinforce them by extending features

4. **Portfolio Value**
   - A finished, deployed app > 3 unfinished "I coded it all myself" projects
   - You can demonstrate: debugging, deployment, architecture decisions
   - The next feature you add (search, CSV import, WebSockets) will be MORE yours

---

## üö´ **When You SHOULD Start Over (Not Your Case)**

Only start a new project if:
- ‚ùå You can't explain how ANY of the code works
- ‚ùå You can't debug issues independently (you CAN - see Render fixes)
- ‚ùå The codebase is fundamentally broken/unsalvageable
- ‚ùå You picked the wrong tech stack for your goals

**None of these apply to you.** Your Render debugging shows you understand Django, deployment, and environment configuration. That's not AI - that's YOU.

---

## üí° **Alternative Approach: Different Project Type**

If you're worried about "too much AI," consider:

**Keep Omni-Stock as your "full-stack showcase"** (what you have now)

**Add a second, smaller project that's more hands-on:**
- Build a CLI tool in Python (less AI-friendly, more manual coding)
- Create a browser extension (unique problem domain)
- Make an open-source contribution to a library you use

**Why this works:**
- Omni-Stock shows: "I can build production apps with modern tools"
- Second project shows: "I can code manually and solve unique problems"
- Together: "I'm versatile and understand when to use AI vs when to code from scratch"

**But honestly:** For a junior role, Omni-Stock alone is ENOUGH if you finish it.

---

## üß™ **HOW TO APPROACH UNIT TESTS WITH AI**

You said: "AI does it very well" - this is TRUE, but here's the nuanced approach:

### **The 70/30 Rule for Tests**

**Let AI Generate: 70%**
- Test scaffolding (class setup, fixtures)
- Happy path tests (basic CRUD operations)
- Basic validation tests
- Standard edge cases (empty strings, null values)

**You Write Manually: 30%**
- Business logic tests (vendor scoping rules)
- Complex permission tests
- Integration tests (multiple models interacting)
- Edge cases specific to YOUR domain

### **Example: Vendor Scoping Tests**

**AI Can Generate:**
```python
def test_user_can_create_collectible(authenticated_client, user):
    response = authenticated_client.post('/api/collectibles/', {
        'name': 'Test Item',
        'quantity': 1
    })
    assert response.status_code == 201
```

**You Should Write:**
```python
def test_user_with_vendor_cannot_create_item_for_different_vendor(
    authenticated_client, user_with_vendor, other_vendor
):
    """
    Critical business rule: Users assigned to a vendor 
    should not be able to create items for a different vendor.
    This test catches a security vulnerability.
    """
    response = authenticated_client.post('/api/collectibles/', {
        'name': 'Test Item',
        'vendor': other_vendor.id  # Try to create for wrong vendor
    })
    assert response.status_code == 403
    assert 'cannot create' in response.json()['detail'].lower()
```

**Why you write this:** AI doesn't understand your BUSINESS RULES. Vendor scoping is specific to Omni-Stock's multi-vendor architecture. You need to think through: "What would break if this didn't work?"

### **The "Explain It Back" Test**

**After AI generates a test, ask yourself:**
1. What is this test verifying?
2. Why would this fail?
3. What real-world bug would this catch?

**If you can't answer all three ‚Üí rewrite the test yourself until you can.**

### **Progressive Approach**

**Week 1:** Let AI generate 90% of tests (learn patterns)
**Week 2:** Generate 70%, manually write complex ones
**Week 3:** Generate 50%, write all business logic tests yourself
**Month 2:** You're writing 70%, AI just helps with boilerplate

**This builds muscle memory while leveraging AI for speed.**

---

## ü§ñ **HOW TO USE AI IN THIS PROJECT GOING FORWARD**

### **The 50/50 Partnership Model**

**AI's Role (50%):**
- Boilerplate generation
- Syntax help (DRF viewset patterns, React Query hooks)
- Test scaffolding
- Documentation formatting
- Debugging suggestions ("try checking ALLOWED_HOSTS")

**Your Role (50%):**
- Architecture decisions ("Should I use WebSockets or polling?")
- Business logic implementation (vendor scoping rules)
- Debugging root causes (WHY is Render returning 404?)
- Code review (Is this AI-generated code correct?)
- Integration testing (Does the whole flow work?)

### **Practical Workflow for Next Features**

**Example: Implementing Search**

**Step 1: Architecture (You Lead)**
- Research: PostgreSQL full-text search vs Elasticsearch
- Decision: Start with `pg_trgm`, migrate to Elasticsearch later
- Write design doc: "Search will use trigram similarity with GIN index"

**Step 2: Boilerplate (AI Assists)**
```python
# You: "Generate a selector for search with trigram similarity"
# AI generates:
from django.contrib.postgres.search import TrigramSimilarity

def search_items(*, user, query):
    qs = Collectible.objects.annotate(
        similarity=TrigramSimilarity('name', query)
    ).filter(similarity__gt=0.3)
    return qs
```

**Step 3: Business Logic (You Add)**
```python
# You add vendor scoping and permission logic:
def search_items(*, user, query):
    qs = Collectible.objects.annotate(
        similarity=TrigramSimilarity('name', query)
    ).filter(similarity__gt=0.3)
    
    # YOUR addition - vendor scoping
    vendor = resolve_user_vendor(user)
    if vendor:
        qs = qs.filter(vendor=vendor)
    
    # YOUR addition - order by relevance
    return qs.order_by('-similarity')
```

**Step 4: Testing (Hybrid)**
```python
# AI generates happy path:
def test_search_returns_matching_items(user):
    create_item(data={'name': 'Pokemon Card', 'user': user})
    results = search_items(user=user, query='Pokemon')
    assert len(results) == 1

# YOU write edge cases:
def test_search_respects_vendor_scoping(user_with_vendor, other_vendor):
    vendor = resolve_user_vendor(user_with_vendor)
    item1 = create_item(data={'name': 'Pokemon', 'vendor': vendor})
    item2 = create_item(data={'name': 'Pokemon', 'vendor': other_vendor})
    
    results = search_items(user=user_with_vendor, query='Pokemon')
    assert item1 in results
    assert item2 not in results  # Critical: vendor scoping works
```

**Step 5: Integration Testing (You Verify)**
- Manually test in browser: "Does search return correct results?"
- Test with production-like data: "Does it handle 1000 items?"
- Performance test: "Is response time <200ms?"

### **The "Could I Rebuild This?" Test**

**After implementing a feature with AI, ask:**
"If I had to rebuild search from scratch, could I?"

**If YES:** You used AI correctly (as a tool)
**If NO:** You over-relied on AI (you didn't learn)

**Fix:** Rebuild one small part manually (e.g., write the search selector without AI help)

---

## üåç **REALITY CHECK: 99% of Devs Use AI**

You're absolutely right - **99% of professional developers use AI in 2025.**

### **How Senior Devs Use AI (What You Should Emulate)**

**Senior Dev at FAANG:**
```
1. Reads requirements
2. Designs architecture on whiteboard
3. Uses Copilot to generate boilerplate
4. Reviews AI code critically ("Is this the best approach?")
5. Manually writes complex business logic
6. Uses AI for tests, but writes edge cases themselves
7. Debugs issues independently (AI can suggest, but senior knows root cause)
```

**Key Difference:** Senior devs use AI as a **junior pair programmer**, not a lead architect.

### **How Juniors Misuse AI**

**Junior (Wrong Way):**
```
1. "Copilot, build me an inventory app"
2. Accepts all suggestions without reading
3. Deploys without understanding
4. Can't debug when something breaks
5. Can't explain why patterns were chosen
```

**This is what hiring managers fear.**

### **How YOU Should Use AI (The Right Way)**

**You (Right Way):**
```
1. "I need a search feature. Research: pg_trgm vs Elasticsearch"
2. "Generate the basic selector with trigram similarity"
3. Review AI code: "Does this handle vendor scoping? No. I'll add that."
4. "Generate happy path tests"
5. Write business logic tests myself (vendor scoping, permissions)
6. Deploy, test manually
7. Debug issues: "Why is search returning other vendor's items? Ah, selector isn't filtering by vendor."
```

**This is the hybrid approach that works.**

---

## üìö **LEARNING STRATEGY: Build Understanding Through AI**

### **The "Explain It to a Rubber Duck" Method**

After AI generates code, explain it out loud:

**AI generates:**
```python
@transaction.atomic
def create_item(*, data, card_details_data=None):
    collectible = Collectible.objects.create(**data)
    if card_details_data:
        CardDetails.objects.create(collectible=collectible, **card_details_data)
    return collectible
```

**You explain:**
"This function creates a collectible and optionally card details in a database transaction. The `@transaction.atomic` decorator means if card details creation fails, the collectible creation is rolled back - we don't end up with orphaned collectibles. This is important because [explain why business logic requires this]."

**If you can't explain it ‚Üí you don't understand it ‚Üí look it up before moving on.**

### **The "Deliberate Practice" Approach**

**Week 1-2: Learn with AI (Current Phase)**
- AI generates 80%, you modify 20%
- Focus: Understanding patterns
- Goal: Learn Django/React idioms

**Week 3-4: Balance (Transition Phase)**
- AI generates 50%, you write 50%
- Focus: Implementing business logic
- Goal: Own the critical paths

**Month 2+: Lead with AI Support**
- You write 70%, AI helps with boilerplate
- Focus: Architecture and complex features
- Goal: AI is your junior assistant

**This is how you "graduate" from AI-heavy to AI-assisted.**

---

## üéì **WHAT TO FOCUS ON NEXT (Prioritized Learning)**

### **Critical Skills for Junior Devs (Practice These Manually)**

1. **Debugging**
   - Next bug: Try to solve WITHOUT AI first
   - Use print statements, Django shell, debugger
   - Only use AI after 30 minutes of manual effort

2. **Reading Docs**
   - Django docs for `select_related` vs `prefetch_related`
   - React Query docs for cache invalidation strategies
   - Docker docs for multi-stage builds
   - **Force yourself to read docs BEFORE asking AI**

3. **Writing Tests**
   - Write one test per feature yourself (no AI)
   - Focus on business logic tests (vendor scoping, permissions)

4. **Code Review**
   - Review all AI-generated code line by line
   - Ask: "Is there a better way?" "What edge cases are missing?"

### **Features to Implement Next (Progressive Difficulty)**

**Easy (Good for Manual Practice):**
1. Add filtering by date range
2. Add sorting options (price, name, date)
3. Add pagination metadata to API responses

**Medium (Good for 50/50 AI Partnership):**
1. Implement search with trigram similarity
2. Add CSV export for inventory list
3. Add bulk delete with confirmation modal

**Hard (Great for Understanding Business Logic):**
1. Implement CSV import with background processing (Celery)
2. Add real-time inventory sync (WebSockets)
3. Add advanced permissions (multi-vendor with team roles)

**Start with Easy, move to Medium, tackle Hard when confident.**

---

## üèÜ **SUCCESS METRICS: Are You Using AI Right?**

### **Green Flags (You're Doing Great)**
- ‚úÖ You can explain architectural decisions without notes
- ‚úÖ You debugged Render issues independently (DisallowedHost, 404)
- ‚úÖ You understand vendor scoping logic
- ‚úÖ You know WHY service/selector pattern was chosen
- ‚úÖ You can modify AI code to fit business needs

### **Yellow Flags (Be Careful)**
- ‚ö†Ô∏è You accept AI suggestions without reading them
- ‚ö†Ô∏è You can't debug without AI's help
- ‚ö†Ô∏è You skip writing tests because "AI does it"
- ‚ö†Ô∏è You avoid reading documentation

### **Red Flags (Over-Reliance)**
- ‚ùå You can't write a function without AI
- ‚ùå You don't understand your own codebase
- ‚ùå You can't explain trade-offs in interviews
- ‚ùå You panic when AI gives wrong suggestions

**Based on your Render debugging and architectural understanding: You're in the GREEN zone.** Keep going.

---

## üéØ **ACTIONABLE PLAN: Next 4 Weeks**

### **Week 1: Finish MVP (AI-Assisted)**
- Deploy frontend to Vercel (AI can help with config)
- Add demo link to README (manual)
- Record demo GIF (manual)
- Fix any deployment bugs (YOU debug, AI suggests)

### **Week 2: Add One Feature Manually**
- **Goal:** Prove to yourself you can code without heavy AI
- **Feature:** Advanced filtering (date range, price range, condition)
- **Method:** 
  - Design: You write plan on paper
  - Implementation: YOU write selector logic
  - Tests: You write business logic tests
  - AI: Only use for boilerplate (viewset setup)

### **Week 3: Add One Complex Feature (50/50)**
- **Feature:** CSV export
- **Method:**
  - You: Design the API endpoint structure
  - AI: Generate basic CSV serialization
  - You: Add vendor scoping and permission checks
  - You: Write tests for edge cases (empty results, special characters)

### **Week 4: Polish & Apply**
- Add screenshots/GIF
- Write blog post about ONE thing you learned (debugging Render, service pattern)
- Practice interview answers
- Start applying

---

## üí∞ **FINAL VERDICT: Investment Analysis**

### **Time Invested So Far**
- ~40-60 hours building Omni-Stock
- Working deployment
- Test coverage
- CI/CD pipeline

### **Options**

**Option A: Start Over (Not Recommended)**
- **Cost:** 40-60 more hours
- **Benefit:** "I coded it all myself" (but so what?)
- **Risk:** You'll hit same deployment issues again
- **Outcome:** SAME portfolio project, just more time wasted

**Option B: Continue with Balanced AI Usage (RECOMMENDED)**
- **Cost:** 20-30 hours to finish MVP + one complex feature
- **Benefit:** Finished, deployed, documented project
- **Risk:** Low - you already solved hard problems
- **Outcome:** Interview-ready portfolio in 3-4 weeks

**Option C: Add Second Hands-On Project (Also Good)**
- **Cost:** 10-15 hours for small CLI/extension project
- **Benefit:** Shows versatility, manual coding skills
- **Risk:** Time investment for diminishing returns
- **Outcome:** "Full-stack + specialized tool" portfolio

**My Recommendation: Option B now, consider Option C in Month 2 if needed.**

---

## üöÄ **THE BOTTOM LINE**

### **Question:** "Should I keep working on this project?"
**Answer:** **YES.** You're 80% done with something interview-worthy. Don't throw that away.

### **Question:** "How should I use AI going forward?"
**Answer:** **50/50 partnership.** AI generates boilerplate, YOU own business logic, architecture, and debugging.

### **Question:** "How do I handle tests?"
**Answer:** **70/30 split.** AI generates happy paths, YOU write complex business logic and edge case tests.

### **Question:** "What about the 99% of devs using AI?"
**Answer:** **Use AI like a senior dev:** as a tool to speed up routine tasks, not as a replacement for understanding. Your Render debugging proves you're on the right track.

---

## üéì **WHAT HIRING MANAGERS ACTUALLY CARE ABOUT**

**They DON'T care:**
- ‚ùå "Did you use AI?" (everyone does)
- ‚ùå "Is every line hand-coded?" (no one does this)

**They DO care:**
- ‚úÖ Can you explain architectural decisions? (YES - you have TECHNICAL_DECISIONS.md)
- ‚úÖ Can you debug complex issues? (YES - Render 404, DisallowedHost)
- ‚úÖ Can you extend the codebase? (PROVE IT - add search or CSV feature)
- ‚úÖ Do you understand trade-offs? (YES - service/selector pattern reasoning)

**You can demonstrate all four.** That's what matters.

---

## üí™ **YOUR COMPETITIVE ADVANTAGE**

Most juniors in 2025 fall into two camps:

**Camp A: "I don't use AI" (Dinosaurs)**
- Slow development
- Resist modern tools
- Pride over pragmatism
- **Hiring managers avoid:** "Will they be productive with our AI-augmented workflows?"

**Camp B: "AI did everything" (Over-reliant)**
- Fast development
- Can't explain decisions
- Panic when AI fails
- **Hiring managers avoid:** "Will they need constant hand-holding?"

**You're in Camp C: "AI-Augmented but Independent"**
- Fast development (AI for boilerplate)
- Can explain decisions (architectural docs)
- Can debug independently (Render issues)
- Understand trade-offs (technical decisions)
- **Hiring managers want:** "They'll be productive and autonomous"

**This is the sweet spot in 2025.** Own it.

---

## üìù **NEXT STEPS (Do This Week)**

1. **Commit to finishing Omni-Stock** (no starting over)
2. **Deploy frontend to Vercel** (get live demo working)
3. **Pick ONE feature to implement with minimal AI** (filtering or sorting)
4. **Write 3 business logic tests manually** (vendor scoping, permissions)
5. **Practice explaining service/selector pattern** (say it out loud 5 times)

**After this week:** You'll have proven to yourself you can code independently. Then leverage AI to BUILD FASTER, not BUILD FOR YOU.

---

**Remember:** AI is a tool, not a crutch. You've already proven you understand the code (debugging Render shows that). Now finish what you started and make it SHINE. üöÄ

**You're not an impostor. You're a pragmatic developer using modern tools effectively.**
